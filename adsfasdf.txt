ASSIGN 1 - Getting data to work with: Download the sample dataset locally for any application (Kaggle)
ï‚· Setting up the working directory.
ï‚· Unpacking the data. Decompress the file locally.
ï‚· Looking at the data. Display the top (10) and bottom (10) of the file.
ï‚· Measuring the length of the data set. Count the number of lines in the file.
ï‚· Encode the categorical data
ï‚· Plot a graph and give your insights for the application selected cases. 

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 1")

getwd()

unzip("archive.zip", exdir = "D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 1")

data <- read.csv("Dimpatient.csv")

head(data, 10)

tail(data,10)

num_lines <- nrow(data)
cat("Number of lines in the dataset:", num_lines, "\n")

data$PatientGender <- as.factor(data$PatientGender)
data$City <- as.factor(data$City)
data$State <- as.factor(data$State)

summary(data)


library(ggplot2)

# Bar plot for PatientGender
gender_plot <- ggplot(data, aes(x = PatientGender)) +
  geom_bar(fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Patients by Gender", x = "Gender", y = "Count") +
  theme_minimal()

# Display the plot
print(gender_plot)


ASSIGN 2.1 - Perform text analysis using R/ Python. 


install.packages("tm") # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
install.packages("ggplot2") # for plotting graphs

library("tm")
library("wordcloud")
library("SnowballC")
library("RColorBrewer")
library("ggplot2")


data <-   read.csv('D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 2\\preprocessed_kindle_review.csv',stringsAsFactors = FALSE)


head(data)

text_column <- data$reviewText
TextDoc <- Corpus(VectorSource(text_column))

toSpace <- content_transformer(function(x, pattern)gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")

TextDoc <- tm_map(TextDoc, content_transformer(tolower))

TextDoc <- tm_map(TextDoc, removeNumbers)

TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))

TextDoc <- tm_map(TextDoc, removeWords,c("kindle", "amazon", "product", "review", "customer",
                                         "one", "two", "three", "four", "five"))

TextDoc <- tm_map(TextDoc, removePunctuation)

TextDoc <- tm_map(TextDoc, stripWhitespace)

TextDoc <- tm_map(TextDoc, stemDocument)

TextDoc_dtm <- TermDocumentMatrix(TextDoc)

dtm_m <- as.matrix(TextDoc_dtm)

dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word=names(dtm_v), freq=dtm_v)

head(dtm_d, 8)

#           word freq
#book       book 15694
#stori     stori 11325
#read       read 10920
#like       like  6583
#charact charact  5763
#just       just  5492
#love       love  5359
#good       good  4411

barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="purple", main ="Top 5 most frequent words",
        ylab = "Word frequencies")

set.seed(1234)

wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40,
          colors=brewer.pal(8, "Dark2"))


ASSIGN 2.2 - Perform data analysis using R programming

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 2.2")

titanic = read.csv("titanic.csv")

head(titanic)

View(titanic)

sapply(titanic, class)

titanic$Survived=as.factor(titanic $Survived) 

titanic $Sex=as.factor(titanic $Sex) 

sapply(titanic, class)

summary(titanic)

sum(is.na(titanic))

dropnull_titanic = titanic[rowSums(is.na(titanic))<=0,]

slist = dropnull_titanic[dropnull_titanic$Survived==1,]

nslist = dropnull_titanic[dropnull_titanic$Survived==0,]

hist(slist$Age, xlab = "Age", ylab = "Freq")

barplot(table(nslist$Sex), xlab = "Gender", ylab = "Freq")

library(tidyverse)

View(mpg)

?mpg

ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))


ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class))

ggplot(mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +
  labs(x = "engine displacement (L)",
       y = "highway fuel efficiency (mpg)",
       color = "type of vehicle",
       title = "Fuel Economy as a Function of Engine Size",
       subtitle = "Fuel Efficiency and Engine Size are Inversely Related",
       caption = "Data obtained from fueleconomy.gov")




ASSIGN 2.3 - Data visualization using R Programming (Consider different input files like sv, excel, JSON etc.) 


# 
# Inclass Assignment:
#   1.	Reading Specific Cells 
# 2.	Skipping Columns

install.packages("readxl")
install.packages("writexl")

library(readxl)
library(writexl)

data <- read_excel("file_show (6).xlsx")
iris <- read_excel("file_show (6).xlsx", sheet = "iris")

bank_full <- read_xlsx("file_show (6).xlsx", sheet = 1)

sdbank_full <- read_xlsx("file_show (6).xlsx", sheet = 1, skip=5)

print(data)

print(iris)

csv_data_salary <- read_xlsx("file_show (4).xlsx")

print(nrow(csv_data_salary))

print(ncol(csv_data_salary))

result <- csv_data_salary[csv_data_salary$salary > 60000, c("name", "salary")]

#create dfs -> then combine into 1 single df-> write this on a csv file

Country <- c("China", "India", "US", "Indonesia", "Pakistan")
Population_1_july_2018 <- c("1,427,647,786", "1,352, 642,280",
                            "327,096,265", "267,670,543", "212, 228,286")
Population_1_july_2019 <- c("1,433,783,686", "1,366,417,754",
                            "329,064,917", "270,625,568", "216, 565,318")
change_in_percents <- c("+0.43%", "+1.02%", "+0.60%", "+1.10%",
                        "+2.04%")


SDF <- data.frame(Country, Population_1_july_2018, Population_1_july_2019, change_in_percents)

write.csv(SDF, "Cpopulation.csv")

library(readr)

read.csv("Cpopulation.csv")


#PLOT
View(iris)

pl <- iris$PetalLengthCm
wl <- iris$PetalWidthCm

plot(pl, wl)

plot(pl, wl, pch=25)

plot(pl, wl, pch=25, col="purple")

library(ggplot2)

ggplot(data=iris) #canvas creation

ggplot(data=iris) + aes(x=PetalLengthCm, y = PetalWidthCm) + geom_point(aes(color = Species, shape = Species))



ASSIGN 3 - Installation of Apache Spark. Writing a program in Spark for Word Count with Unit
Tests: Change Application

COLAB

#upload this file
# Extract the package
!tar -xvf "/content/drive/MyDrive/Colab Notebooks/BDA/spark-3.5.1-bin-hadoop3.tgz"
!pip install -q findspark
import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

print("Spark session created!")
from pyspark import SparkContext

# Sample text
text_data = ["Apache Spark is a fast and general-purpose cluster computing system.",
             "It provides high-level APIs in Java, Scala, Python, and R.",
             "Spark also supports a rich set of higher-level tools."]

# Create SparkContext
sc = spark.sparkContext

# Word count logic
rdd = sc.parallelize(text_data)
word_counts = (
    rdd.flatMap(lambda line: line.split())
       .map(lambda word: (word.lower(), 1))
       .reduceByKey(lambda a, b: a + b)
)

# Collect and print word counts
result = word_counts.collect()
print("Word counts:", result)

import unittest

class WordCountTest(unittest.TestCase):
    def setUp(self):
        # Set up SparkContext
        self.sc = sc

    def test_word_count(self):
        # Input data
        test_data = ["hello world", "hello Spark"]
        rdd = self.sc.parallelize(test_data)

        # Word count logic
        word_counts = (
            rdd.flatMap(lambda line: line.split())
               .map(lambda word: (word.lower(), 1))
               .reduceByKey(lambda a, b: a + b)
        )
        result = dict(word_counts.collect())

        # Expected result
        expected = {"hello": 2, "world": 1, "spark": 1}
        self.assertEqual(result, expected)

# Run the tests
unittest.main(argv=[''], verbosity=2, exit=False)


VSCODE

import os

os.environ["JAVA_HOME"] = "C:\Program Files\Java\jdk1.8.0_202"
os.environ["SPARK_HOME"] = "C:\spark-3.5.1-bin-hadoop3"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

print("Spark session created!")
from pyspark import SparkContext

# Sample text
text_data = ["Apache Spark is a fast and general-purpose cluster computing system.",
             "It provides high-level APIs in Java, Scala, Python, and R.",
             "Spark also supports a rich set of higher-level tools."]

# Create SparkContext
sc = spark.sparkContext

# Word count logic
rdd = sc.parallelize(text_data)
word_counts = (
    rdd.flatMap(lambda line: line.split())
       .map(lambda word: (word.lower(), 1))
       .reduceByKey(lambda a, b: a + b)
)

# Collect and print word counts
result = word_counts.collect()
print("Word counts:", result, end=" ")

import unittest

class WordCountTest(unittest.TestCase):
    def setUp(self):
        # Set up SparkContext
        self.sc = sc

    def test_word_count(self):
        # Input data
        test_data = ["hello world", "hello Spark"]
        rdd = self.sc.parallelize(test_data)

        # Word count logic
        word_counts = (
            rdd.flatMap(lambda line: line.split())
               .map(lambda word: (word.lower(), 1))
               .reduceByKey(lambda a, b: a + b)
        )
        result = dict(word_counts.collect())

        # Expected result
        expected = {"hello": 2, "world": 1, "spark": 1}
        self.assertEqual(result, expected)

# Run the tests
unittest.main(argv=[''], verbosity=2, exit=False)



ASSIGN 4 - PySpark - Read CSV file into Data Frame Create and query a HIVE table in PySpark.

COLAB


# Install Java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf "/content/drive/MyDrive/Colab Notebooks/BDA/spark-3.5.1-bin-hadoop3.tgz"

# Set environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"
# Install PySpark
!pip install -q pyspark findspark
import findspark
findspark.init()

from pyspark.sql import SparkSession

# Initialize Spark session with Hive support
spark = SparkSession.builder \
    .appName("PySpark Hive Example") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()

print("Spark session created with Hive support.")
# Replace 'your_file.csv' with the actual filename
file_name = "/content/ds_salaries.csv"

# Read the CSV file into a DataFrame
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
# Create a temporary view
df.createOrReplaceTempView("temp_table")

# Save the table as a Hive table
spark.sql("CREATE TABLE hive_table AS SELECT * FROM temp_table")

# Query the Hive table
query_result = spark.sql("SELECT * FROM hive_table")
query_result.show()
# Example query
spark.sql("SELECT COUNT(*) FROM hive_table").show()


VSCODE

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("PySpark Hive Example") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()
print("Spark session created with Hive support.")
# Load CSV into DataFrame
file_path = "D:\MIT ADT\LY - Sem 1\BDA Lab\Sir\Assign 4\ds_salaries.csv"  # Update with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)
# Show the data
print("CSV Data:")
df.show()
""" # Create a temporary view
df.createOrReplaceTempView("temp_table")

# Create and query Hive table
spark.sql("CREATE TABLE hive_table AS SELECT * FROM temp_table")
result = spark.sql("SELECT * FROM hive_table") """
# Try creating a normal Spark SQL table (without Hive support)
spark.sql("CREATE OR REPLACE TEMP VIEW hive_table AS SELECT * FROM temp_table")
# Query the data
result = spark.sql("SELECT * FROM hive_table")
print("Hive Table Data:")
result.show()


ASSIGN 5 - To implement the K-means algorithm and show the results graphically (using R
programming). (Here the students are expected to use R programming and comment on the
value of k selected for the application and analyze the output. (why and how the k value is
selected))

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Sir\\Assign 5")

# Install the required libraries if not already installed
install.packages("ggplot2")
install.packages("factoextra")
install.packages("dplyr")

install.packages("colorspace")
# Load the required libraries
library(ggplot2)
library(factoextra)
library(dplyr)

#Load the Mall Customers Dataset
mall_customers = read.csv("Mall_Customers.csv")
head(mall_customers)

names(mall_customers)
print(colnames(mall_customers))

data <- mall_customers[c("Annual.Income","Spending.Score")]
head(data)

# Scale the data (optional, but recommended for K-Means)
data_scaled <- scale(data)
print("scaled data")
head(data_scaled)
# calculated WSS for each clustes from 1 to 15
wss <- numeric(15)
for (k in 1:15) wss[k] <- sum(kmeans(data_scaled, centers=k,
                                     nstart=25)$withinss)

# plot the graph of number of clusters vs WSS
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="WSS")
#Plots both points and lines. The points are plotted at each data value, and theyâ€™re connected by lines.
# Elbow method to find the optimal number of clusters
fviz_nbclust(data_scaled, kmeans, method = "wss") + 
  geom_vline(xintercept = 5, linetype = 2) +
  labs(subtitle = "Elbow method")


# Silhouette method
fviz_nbclust(data_scaled, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
  
# Apply K-Means clustering with 3 clusters
set.seed(123)  # Set seed for reproducibility in case python- random_state
kmeans_result <- kmeans(data_scaled, centers = 5, nstart = 25)

# Add the cluster assignments to the original dataset
mall_customers$Cluster <- as.factor(kmeans_result$cluster)
tail(mall_customers)

# Scatter plot of the clusters
ggplot(mall_customers, aes(x = Annual.Income, y = Spending.Score, color = Cluster)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green","yellow","black")) +
  labs(title = "K-Means Clustering of Mall Customers",
       x = "Annual Income (k$)",
       y = "Spending Score (1-100)") +
  theme_minimal()

# Scatter plot with cluster centers
fviz_cluster(kmeans_result, data = data_scaled,
             geom = "point",
             ellipse.type = "norm",
             ggtheme = theme_minimal(),
             palette = c("red", "blue", "green","yellow","black"))
kmeans_result


ASSIGN 6 - Compute TF-IDF values of words i) From a corpus having unique values. ii) From a corpus having similar documents iii) A single word repeated multiple times in multiple documents.


import nltk
from nltk.corpus import stopwords
from math import log
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Download stopwords list if not already downloaded
nltk.download('stopwords')
# Define stop words
stop_words = set(stopwords.words('english'))
# Step 1: Tokenization with stop word removal
def tokenize(doc):
    return [word.lower() for word in doc.split() if word.lower() not in stop_words]

    # Example usage of `tokenize`
# Input: "The car is driven on the road"
# Output: ['car', 'driven', 'road']
# Step 2: Create vocabulary (all unique terms excluding stop words)
def create_vocabulary(corpus):
    return list(set(word for doc in corpus for word in tokenize(doc)))

# Example usage of `create_vocabulary`
# Input: ["apple orange banana", "grape pineapple mango", "peach plum berry"]
# Output: ['plum', 'banana', 'apple', 'grape', 'berry', 'orange', 'peach', 'pineapple']
# Step 3: Calculate Term Frequency (TF) for all terms in each document
def term_frequency_matrix(corpus, vocabulary):
    tf_matrix = []
    for doc in corpus:
        tf_vector = []
        doc_tokens = tokenize(doc)
        for term in vocabulary:
            tf = doc_tokens.count(term) / len(doc_tokens)  # Relative frequency
            tf_vector.append(tf)
        tf_matrix.append(tf_vector)
    return tf_matrix

# Example usage of `term_frequency_matrix`
# Input: ["apple orange banana", "apple apple banana"], ['apple', 'orange', 'banana']
# Output: [[0.3333, 0.3333, 0.3333], [0.6667, 0, 0.3333]]
# Step 4: Calculate Document Frequency (DF) for each term
def document_frequency(term, corpus):
    return sum(1 for doc in corpus if term in tokenize(doc))

# Example usage of `document_frequency`
# Input: 'apple', ["apple orange banana", "apple apple banana", "grape pineapple"]
# Output: 2
# Step 5: Calculate Inverse Document Frequency (IDF) for each term
def inverse_document_frequency_matrix(corpus, vocabulary):
    N = len(corpus)
    idf_vector = []
    for term in vocabulary:
        df = document_frequency(term, corpus)
        idf = log(N / df) if df != 0 else 0  # Avoid division by zero
        idf_vector.append(idf)
    return idf_vector

# Example usage of `inverse_document_frequency_matrix`
# Input: ["apple orange banana", "apple apple banana", "grape pineapple"], ['apple', 'orange', 'banana', 'grape', 'pineapple']
# Output: [0.4055, 1.0986, 0.4055, 1.0986, 1.0986]
# Step 6: Calculate TF-IDF matrix by multiplying TF and IDF
def tfidf_matrix(tf_matrix, idf_vector):
    tfidf_matrix = []
    for tf_vector in tf_matrix:
        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]
        tfidf_matrix.append(tfidf_vector)
    return tfidf_matrix

# Example usage of `tfidf_matrix`
# Input: [[0.3333, 0.3333, 0.3333], [0.6667, 0, 0.3333]], [0.4055, 1.0986, 0.4055]
# Output: [[0.1352, 0.3662, 0.1352], [0.2703, 0.0, 0.1352]]
# Heatmap visualization
def plot_heatmap(df, title):
    plt.figure(figsize=(10, 6))
    sns.heatmap(df, annot=True, fmt=".2f", cmap="plasma", cbar=True)
    plt.title(title, fontsize=16)
    plt.xlabel("Terms", fontsize=12)
    plt.ylabel("Documents", fontsize=12)
    plt.show()
# Experiment Scenarios
corpora = {
    "Unique Values": [
        "apple orange banana",
        "grape pineapple mango",
        "peach plum berry"
    ],
    "Similar Documents": [
        "car road fast",
        "car road blue",
        "car road driven"
    ],
    "Repeated Word": [
        "car car car car car",
        "car car driven road",
        "car car highway car car"
    ]
}
# Perform calculations for each corpus
for scenario, corpus in corpora.items():
    print(f"\n--- Scenario: {scenario} ---\n")

    # Create vocabulary
    vocabulary = create_vocabulary(corpus)

    # Calculate TF, IDF, and TF-IDF
    tf_matrix = term_frequency_matrix(corpus, vocabulary)
    idf_vector = inverse_document_frequency_matrix(corpus, vocabulary)
    tfidf = tfidf_matrix(tf_matrix, idf_vector)

    # Convert to DataFrames for display
    df_tf = pd.DataFrame(tf_matrix, columns=vocabulary)
    df_idf = pd.DataFrame([idf_vector], columns=vocabulary)
    df_tfidf = pd.DataFrame(tfidf, columns=vocabulary)

    # Display data
    print("Vocabulary:", vocabulary)
    print("\nTerm Frequency (TF):")
    print(df_tf)
    print("\nInverse Document Frequency (IDF):")
    print(df_idf)
    print("\nTF-IDF:")
    print(df_tfidf)

    # Visualize TF-IDF matrix
    plot_heatmap(df_tfidf, f"TF-IDF Heatmap - {scenario}")



ASSIGN 7 - Analytical representation of Linear regression using Movie  ecommendation dataset


# #movies_data <- data.frame(
# #  MovieID = 1:10,
# #  Genre = factor(c("Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action")),
# #  Budget = c(15000000, 5000000, 10000000, 20000000, 6000000, 12000000, 18000000, 5500000, 11000000, 16000000),
# #  Rating = c(7.8, 6.5, 8.2, 7.9, 6.9, 8.0, 7.5, 6.7, 8.1, 7.7)
# #)
# 
# movies_data <-load("movies.Rdata")
# #write.csv(movies_data, file = "movies_data.csv", row.names = FALSE)
# 
# print(movies_data)
# 
# install.packages("ggplot2")
# install.packages("dplyr")
# 
# library(ggplot2)
# library(dplyr)
# 
# 
# head(movies)
# 
# str(movies)
# 
# movies$genre <- as.factor(movies$genre)
# 
# summary(movies)
# 
# set.seed(123)
# 
# # Create a training set (70%) and test set (30%)
# sample_indices <- sample(seq_len(nrow(movies)), size = 0.7 * nrow(movies))
# train_set <- movies[sample_indices, ]
# test_set <- movies[-sample_indices, ]
# 
# model <- lm(imdb_rating ~ genre, data = train_set)
# 
# # Summary of the model
# summary(model)
# 
# predictions <- predict(model, newdata = test_set)
# 
# # Combine predictions with actual values
# results <- data.frame(Actual = test_set$imdb_rating, Predicted = predictions)
# 
# # Calculate Mean Squared Error
# mse <- mean((results$Actual - results$Predicted)^2)
# print(paste("Mean Squared Error:", mse))
# 
# 
# ggplot(results, aes(x = Actual, y = Predicted)) +
#   geom_point() +
#   geom_abline(intercept = 0, slope = 1, color = "red") +
#   labs(title = "Actual vs Predicted Ratings", x = "Actual Rating", y = "Predicted Rating")
# 
# 
# residuals <- results$Actual - results$Predicted
# 
# ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
#   geom_histogram(bins = 30, fill = "blue", color = "black") +
#   labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency")
# 
# 
# 
# 

# Load required libraries
install.packages("ggplot2", dependencies = TRUE)
install.packages("dplyr", dependencies = TRUE)

library(ggplot2)
library(dplyr)

# Load dataset
load("movies.Rdata")

# Inspect the data
str(movies)
summary(movies)

# Ensure Genre is a factor
movies$genre <- as.factor(movies$genre)

# Split data into training and test sets (70/30 split)
set.seed(123)
sample_indices <- sample(seq_len(nrow(movies)), size = 0.7 * nrow(movies))
train_set <- movies[sample_indices, ]
test_set <- movies[-sample_indices, ]

# Build the linear regression model
model <- lm(imdb_rating ~ genre, data = train_set)

# Model summary
summary(model)

# Predict on test set
predictions <- predict(model, newdata = test_set)

# Combine actual and predicted values
results <- data.frame(Actual = test_set$imdb_rating, Predicted = predictions)

# Evaluate model performance
mse <- mean((results$Actual - results$Predicted)^2)
rmse <- sqrt(mse)
mae <- mean(abs(results$Actual - results$Predicted))

cat("Model Evaluation Metrics:\n")
cat(paste("Mean Squared Error (MSE):", round(mse, 3), "\n"))
cat(paste("Root Mean Squared Error (RMSE):", round(rmse, 3), "\n"))
cat(paste("Mean Absolute Error (MAE):", round(mae, 3), "\n"))

# Plot Actual vs Predicted ratings
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted Ratings", x = "Actual Rating", y = "Predicted Rating") +
  theme_minimal()

# Residual analysis
residuals <- results$Actual - results$Predicted

# Histogram of residuals
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Residuals vs Fitted Values plot for diagnostics
ggplot(data.frame(Fitted = predictions, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "darkorange", alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

#LR USING MTCARS

# Load required libraries
install.packages("ggplot2", dependencies = TRUE)
install.packages("dplyr", dependencies = TRUE)

library(ggplot2)
library(dplyr)

# Load the mtcars dataset
data(mtcars)

# Inspect the dataset
str(mtcars)
summary(mtcars)

# Split data into training (70%) and test (30%) sets
set.seed(123)
sample_indices <- sample(seq_len(nrow(mtcars)), size = 0.7 * nrow(mtcars))
train_set <- mtcars[sample_indices, ]
test_set <- mtcars[-sample_indices, ]

# Build the linear regression model to predict mpg
model <- lm(mpg ~ hp + wt + qsec, data = train_set)

# Model summary
summary(model)

# Predict on test set
predictions <- predict(model, newdata = test_set)

# Combine actual and predicted values
results <- data.frame(Actual = test_set$mpg, Predicted = predictions)

# Evaluate model performance
mse <- mean((results$Actual - results$Predicted)^2)
rmse <- sqrt(mse)
mae <- mean(abs(results$Actual - results$Predicted))

cat("Model Evaluation Metrics:\n")
cat(paste("Mean Squared Error (MSE):", round(mse, 3), "\n"))
cat(paste("Root Mean Squared Error (RMSE):", round(rmse, 3), "\n"))
cat(paste("Mean Absolute Error (MAE):", round(mae, 3), "\n"))

# Plot Actual vs Predicted mpg values
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted MPG", x = "Actual MPG", y = "Predicted MPG") +
  theme_minimal()

# Residual analysis
residuals <- results$Actual - results$Predicted

# Histogram of residuals
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Residuals vs Fitted Values plot for diagnostics
ggplot(data.frame(Fitted = predictions, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "darkorange", alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
  theme_minimal()



ASSIGN 8 - Data streaming and pipelining using online cloud platform using confluent for Kafka cloud

https://www.youtube.com/watch?v=oI7VAS9KSS4

https://drive.google.com/drive/folders/16vkmCspZoAIix9vr5aBcBTK6dX-JJ5gL?usp=sharing


ASSIGN 9 - Social Network Analysis using R (for example: Community Detection Algorithm)

install.packages("igraph")
install.packages("sna")

library(igraph)
library(network)
library(sna)

plot(make_full_graph(8, directed=FALSE))

Ring_Graph <- make_ring(12, directed = FALSE, mutual =FALSE, circular = TRUE)
# try for False
plot(Ring_Graph)

Star_Graph <- make_star(12, center = 4)
plot(Star_Graph)

gnp_Graph <- sample_gnp(20, 0.5, directed = FALSE, loops =FALSE)
plot(gnp_Graph)

gnp_Graph1 <- sample_gnp(7, 0.4, directed = FALSE, loops = FALSE)
plot(gnp_Graph1)

node_degrees <- degree(gnp_Graph)
print(node_degrees)

sample_graph <- sample_gnp(10, 0.3, directed = FALSE)
plot(sample_graph)
sample_density <- edge_density(sample_graph, loops = FALSE)
sample_density

sample_graph <- sample_gnp(20, 0.3, directed = FALSE, loops= FALSE)
plot(sample_graph)
clique_num(sample_graph)

components(sample_graph)


setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 9")
data <- read.csv("socialnetworkdata.csv")
y <- data.frame(data$first, data$second)
net <- graph.data.frame(y, directed=T)
V(net)
E(net)
plot(net)

hist(degree(net), col='purple', main = "histogram of node degree",ylab = 'freq', xlab = 'vertices degree')


set.seed(222)
plot(net,
     vertex.color = 'cyan',
     vertext.size = 2,
     edge.arrow.size = 0.1,
     vertex.label.cex = 0.8)


plot(net,
     vertex.color = rainbow(vcount(net)),  # Use the number of vertices to generate colors
     vertex.size = degree(net) * 0.4,      # Calculate vertex degrees and scale their size
     edge.arrow.size = 0.1,                # Keep arrow size for edges
     layout = layout.fruchterman.reingold)


hs <- hub_score(net)$vector
hs
as <- authority_score(net)$vector
as
set.seed(123)
plot(net,
     vertex.size=hs*30,
     main = 'Hubs',
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai)


undirected_net <- as.undirected(net, mode = "collapse")

community <- cluster_louvain(undirected_net)

plot(undirected_net,
     vertex.color = membership(community),  # Color vertices by community membership
     vertex.size = degree(undirected_net) * 0.4,  # Size vertices by degree
     edge.arrow.size = 0.1,  # Edge arrow size
     main = "Community Detection using Louvain Method")


# Initialization: Assign each node to its own community.
# Local Moving: For each node, attempt to move it to neighboring communities if it increases modularity (a measure of community structure quality).
# Community Aggregation: Treat each community as a "supernode" and create a new graph with these supernodes as nodes, where edge weights represent the strength of connections between communities.
# Repeat: Iterate the moving and aggregation steps on the new graph until modularity no longer increases.
# Output: The final community structure, with nodes grouped into communities maximizing modularity.


ðŸ¤¡







