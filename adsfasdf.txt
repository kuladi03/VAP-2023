ASSIGN 1 - Getting data to work with: Download the sample dataset locally for any application (Kaggle)
 Setting up the working directory.
 Unpacking the data. Decompress the file locally.
 Looking at the data. Display the top (10) and bottom (10) of the file.
 Measuring the length of the data set. Count the number of lines in the file.
 Encode the categorical data
 Plot a graph and give your insights for the application selected cases. 

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 1")

getwd()

unzip("archive.zip", exdir = "D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 1")

data <- read.csv("Dimpatient.csv")

head(data, 10)

tail(data,10)

num_lines <- nrow(data)
cat("Number of lines in the dataset:", num_lines, "\n")

data$PatientGender <- as.factor(data$PatientGender)
data$City <- as.factor(data$City)
data$State <- as.factor(data$State)

summary(data)


library(ggplot2)

# Bar plot for PatientGender
gender_plot <- ggplot(data, aes(x = PatientGender)) +
  geom_bar(fill = "lightgreen", color = "black") +
  labs(title = "Distribution of Patients by Gender", x = "Gender", y = "Count") +
  theme_minimal()

# Display the plot
print(gender_plot)


ASSIGN 2.1 - Perform text analysis using R/ Python. 


install.packages("tm") # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
install.packages("ggplot2") # for plotting graphs

library("tm")
library("wordcloud")
library("SnowballC")
library("RColorBrewer")
library("ggplot2")


data <-   read.csv('D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 2\\preprocessed_kindle_review.csv',stringsAsFactors = FALSE)


head(data)

text_column <- data$reviewText
TextDoc <- Corpus(VectorSource(text_column))

toSpace <- content_transformer(function(x, pattern)gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")

TextDoc <- tm_map(TextDoc, content_transformer(tolower))

TextDoc <- tm_map(TextDoc, removeNumbers)

TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))

TextDoc <- tm_map(TextDoc, removeWords,c("kindle", "amazon", "product", "review", "customer",
                                         "one", "two", "three", "four", "five"))

TextDoc <- tm_map(TextDoc, removePunctuation)

TextDoc <- tm_map(TextDoc, stripWhitespace)

TextDoc <- tm_map(TextDoc, stemDocument)

TextDoc_dtm <- TermDocumentMatrix(TextDoc)

dtm_m <- as.matrix(TextDoc_dtm)

dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word=names(dtm_v), freq=dtm_v)

head(dtm_d, 8)

#           word freq
#book       book 15694
#stori     stori 11325
#read       read 10920
#like       like  6583
#charact charact  5763
#just       just  5492
#love       love  5359
#good       good  4411

barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="purple", main ="Top 5 most frequent words",
        ylab = "Word frequencies")

set.seed(1234)

wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40,
          colors=brewer.pal(8, "Dark2"))


ASSIGN 2.2 - Perform data analysis using R programming

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 2.2")

titanic = read.csv("titanic.csv")

head(titanic)

View(titanic)

sapply(titanic, class)

titanic$Survived=as.factor(titanic $Survived) 

titanic $Sex=as.factor(titanic $Sex) 

sapply(titanic, class)

summary(titanic)

sum(is.na(titanic))

dropnull_titanic = titanic[rowSums(is.na(titanic))<=0,]

slist = dropnull_titanic[dropnull_titanic$Survived==1,]

nslist = dropnull_titanic[dropnull_titanic$Survived==0,]

hist(slist$Age, xlab = "Age", ylab = "Freq")

barplot(table(nslist$Sex), xlab = "Gender", ylab = "Freq")

library(tidyverse)

View(mpg)

?mpg

ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))


ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class))

ggplot(mpg) +
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +
  labs(x = "engine displacement (L)",
       y = "highway fuel efficiency (mpg)",
       color = "type of vehicle",
       title = "Fuel Economy as a Function of Engine Size",
       subtitle = "Fuel Efficiency and Engine Size are Inversely Related",
       caption = "Data obtained from fueleconomy.gov")




ASSIGN 2.3 - Data visualization using R Programming (Consider different input files like sv, excel, JSON etc.) 


# 
# Inclass Assignment:
#   1.	Reading Specific Cells 
# 2.	Skipping Columns

install.packages("readxl")
install.packages("writexl")

library(readxl)
library(writexl)

data <- read_excel("file_show (6).xlsx")
iris <- read_excel("file_show (6).xlsx", sheet = "iris")

bank_full <- read_xlsx("file_show (6).xlsx", sheet = 1)

sdbank_full <- read_xlsx("file_show (6).xlsx", sheet = 1, skip=5)

print(data)

print(iris)

csv_data_salary <- read_xlsx("file_show (4).xlsx")

print(nrow(csv_data_salary))

print(ncol(csv_data_salary))

result <- csv_data_salary[csv_data_salary$salary > 60000, c("name", "salary")]

#create dfs -> then combine into 1 single df-> write this on a csv file

Country <- c("China", "India", "US", "Indonesia", "Pakistan")
Population_1_july_2018 <- c("1,427,647,786", "1,352, 642,280",
                            "327,096,265", "267,670,543", "212, 228,286")
Population_1_july_2019 <- c("1,433,783,686", "1,366,417,754",
                            "329,064,917", "270,625,568", "216, 565,318")
change_in_percents <- c("+0.43%", "+1.02%", "+0.60%", "+1.10%",
                        "+2.04%")


SDF <- data.frame(Country, Population_1_july_2018, Population_1_july_2019, change_in_percents)

write.csv(SDF, "Cpopulation.csv")

library(readr)

read.csv("Cpopulation.csv")


#PLOT
View(iris)

pl <- iris$PetalLengthCm
wl <- iris$PetalWidthCm

plot(pl, wl)

plot(pl, wl, pch=25)

plot(pl, wl, pch=25, col="purple")

library(ggplot2)

ggplot(data=iris) #canvas creation

ggplot(data=iris) + aes(x=PetalLengthCm, y = PetalWidthCm) + geom_point(aes(color = Species, shape = Species))



ASSIGN 3 - Installation of Apache Spark. Writing a program in Spark for Word Count with Unit
Tests: Change Application

COLAB

#upload this file
# Extract the package
!tar -xvf "/content/drive/MyDrive/Colab Notebooks/BDA/spark-3.5.1-bin-hadoop3.tgz"
!pip install -q findspark
import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

print("Spark session created!")
from pyspark import SparkContext

# Sample text
text_data = ["Apache Spark is a fast and general-purpose cluster computing system.",
             "It provides high-level APIs in Java, Scala, Python, and R.",
             "Spark also supports a rich set of higher-level tools."]

# Create SparkContext
sc = spark.sparkContext

# Word count logic
rdd = sc.parallelize(text_data)
word_counts = (
    rdd.flatMap(lambda line: line.split())
       .map(lambda word: (word.lower(), 1))
       .reduceByKey(lambda a, b: a + b)
)

# Collect and print word counts
result = word_counts.collect()
print("Word counts:", result)

import unittest

class WordCountTest(unittest.TestCase):
    def setUp(self):
        # Set up SparkContext
        self.sc = sc

    def test_word_count(self):
        # Input data
        test_data = ["hello world", "hello Spark"]
        rdd = self.sc.parallelize(test_data)

        # Word count logic
        word_counts = (
            rdd.flatMap(lambda line: line.split())
               .map(lambda word: (word.lower(), 1))
               .reduceByKey(lambda a, b: a + b)
        )
        result = dict(word_counts.collect())

        # Expected result
        expected = {"hello": 2, "world": 1, "spark": 1}
        self.assertEqual(result, expected)

# Run the tests
unittest.main(argv=[''], verbosity=2, exit=False)


VSCODE

import os

os.environ["JAVA_HOME"] = "C:\Program Files\Java\jdk1.8.0_202"
os.environ["SPARK_HOME"] = "C:\spark-3.5.1-bin-hadoop3"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WordCount") \
    .getOrCreate()

print("Spark session created!")
from pyspark import SparkContext

# Sample text
text_data = ["Apache Spark is a fast and general-purpose cluster computing system.",
             "It provides high-level APIs in Java, Scala, Python, and R.",
             "Spark also supports a rich set of higher-level tools."]

# Create SparkContext
sc = spark.sparkContext

# Word count logic
rdd = sc.parallelize(text_data)
word_counts = (
    rdd.flatMap(lambda line: line.split())
       .map(lambda word: (word.lower(), 1))
       .reduceByKey(lambda a, b: a + b)
)

# Collect and print word counts
result = word_counts.collect()
print("Word counts:", result, end=" ")

import unittest

class WordCountTest(unittest.TestCase):
    def setUp(self):
        # Set up SparkContext
        self.sc = sc

    def test_word_count(self):
        # Input data
        test_data = ["hello world", "hello Spark"]
        rdd = self.sc.parallelize(test_data)

        # Word count logic
        word_counts = (
            rdd.flatMap(lambda line: line.split())
               .map(lambda word: (word.lower(), 1))
               .reduceByKey(lambda a, b: a + b)
        )
        result = dict(word_counts.collect())

        # Expected result
        expected = {"hello": 2, "world": 1, "spark": 1}
        self.assertEqual(result, expected)

# Run the tests
unittest.main(argv=[''], verbosity=2, exit=False)



ASSIGN 4 - PySpark - Read CSV file into Data Frame Create and query a HIVE table in PySpark.

COLAB


# Install Java
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!tar xf "/content/drive/MyDrive/Colab Notebooks/BDA/spark-3.5.1-bin-hadoop3.tgz"

# Set environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.1-bin-hadoop3"
# Install PySpark
!pip install -q pyspark findspark
import findspark
findspark.init()

from pyspark.sql import SparkSession

# Initialize Spark session with Hive support
spark = SparkSession.builder \
    .appName("PySpark Hive Example") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()

print("Spark session created with Hive support.")
# Replace 'your_file.csv' with the actual filename
file_name = "/content/ds_salaries.csv"

# Read the CSV file into a DataFrame
df = spark.read.csv(file_name, header=True, inferSchema=True)
df.show()
# Create a temporary view
df.createOrReplaceTempView("temp_table")

# Save the table as a Hive table
spark.sql("CREATE TABLE hive_table AS SELECT * FROM temp_table")

# Query the Hive table
query_result = spark.sql("SELECT * FROM hive_table")
query_result.show()
# Example query
spark.sql("SELECT COUNT(*) FROM hive_table").show()


VSCODE

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("PySpark Hive Example") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()
print("Spark session created with Hive support.")
# Load CSV into DataFrame
file_path = "D:\MIT ADT\LY - Sem 1\BDA Lab\Sir\Assign 4\ds_salaries.csv"  # Update with your file path
df = spark.read.csv(file_path, header=True, inferSchema=True)
# Show the data
print("CSV Data:")
df.show()
""" # Create a temporary view
df.createOrReplaceTempView("temp_table")

# Create and query Hive table
spark.sql("CREATE TABLE hive_table AS SELECT * FROM temp_table")
result = spark.sql("SELECT * FROM hive_table") """
# Try creating a normal Spark SQL table (without Hive support)
spark.sql("CREATE OR REPLACE TEMP VIEW hive_table AS SELECT * FROM temp_table")
# Query the data
result = spark.sql("SELECT * FROM hive_table")
print("Hive Table Data:")
result.show()


ASSIGN 5 - To implement the K-means algorithm and show the results graphically (using R
programming). (Here the students are expected to use R programming and comment on the
value of k selected for the application and analyze the output. (why and how the k value is
selected))

setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Sir\\Assign 5")

# Install the required libraries if not already installed
install.packages("ggplot2")
install.packages("factoextra")
install.packages("dplyr")

install.packages("colorspace")
# Load the required libraries
library(ggplot2)
library(factoextra)
library(dplyr)

#Load the Mall Customers Dataset
mall_customers = read.csv("Mall_Customers.csv")
head(mall_customers)

names(mall_customers)
print(colnames(mall_customers))

data <- mall_customers[c("Annual.Income","Spending.Score")]
head(data)

# Scale the data (optional, but recommended for K-Means)
data_scaled <- scale(data)
print("scaled data")
head(data_scaled)
# calculated WSS for each clustes from 1 to 15
wss <- numeric(15)
for (k in 1:15) wss[k] <- sum(kmeans(data_scaled, centers=k,
                                     nstart=25)$withinss)

# plot the graph of number of clusters vs WSS
plot(1:15, wss, type="b", xlab="Number of Clusters", ylab="WSS")
#Plots both points and lines. The points are plotted at each data value, and they’re connected by lines.
# Elbow method to find the optimal number of clusters
fviz_nbclust(data_scaled, kmeans, method = "wss") + 
  geom_vline(xintercept = 5, linetype = 2) +
  labs(subtitle = "Elbow method")


# Silhouette method
fviz_nbclust(data_scaled, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
  
# Apply K-Means clustering with 3 clusters
set.seed(123)  # Set seed for reproducibility in case python- random_state
kmeans_result <- kmeans(data_scaled, centers = 5, nstart = 25)

# Add the cluster assignments to the original dataset
mall_customers$Cluster <- as.factor(kmeans_result$cluster)
tail(mall_customers)

# Scatter plot of the clusters
ggplot(mall_customers, aes(x = Annual.Income, y = Spending.Score, color = Cluster)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green","yellow","black")) +
  labs(title = "K-Means Clustering of Mall Customers",
       x = "Annual Income (k$)",
       y = "Spending Score (1-100)") +
  theme_minimal()

# Scatter plot with cluster centers
fviz_cluster(kmeans_result, data = data_scaled,
             geom = "point",
             ellipse.type = "norm",
             ggtheme = theme_minimal(),
             palette = c("red", "blue", "green","yellow","black"))
kmeans_result


ASSIGN 6 - Compute TF-IDF values of words i) From a corpus having unique values. ii) From a corpus having similar documents iii) A single word repeated multiple times in multiple documents.


import nltk
from nltk.corpus import stopwords
from math import log
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Download stopwords list if not already downloaded
nltk.download('stopwords')
# Define stop words
stop_words = set(stopwords.words('english'))
# Step 1: Tokenization with stop word removal
def tokenize(doc):
    return [word.lower() for word in doc.split() if word.lower() not in stop_words]

    # Example usage of `tokenize`
# Input: "The car is driven on the road"
# Output: ['car', 'driven', 'road']
# Step 2: Create vocabulary (all unique terms excluding stop words)
def create_vocabulary(corpus):
    return list(set(word for doc in corpus for word in tokenize(doc)))

# Example usage of `create_vocabulary`
# Input: ["apple orange banana", "grape pineapple mango", "peach plum berry"]
# Output: ['plum', 'banana', 'apple', 'grape', 'berry', 'orange', 'peach', 'pineapple']
# Step 3: Calculate Term Frequency (TF) for all terms in each document
def term_frequency_matrix(corpus, vocabulary):
    tf_matrix = []
    for doc in corpus:
        tf_vector = []
        doc_tokens = tokenize(doc)
        for term in vocabulary:
            tf = doc_tokens.count(term) / len(doc_tokens)  # Relative frequency
            tf_vector.append(tf)
        tf_matrix.append(tf_vector)
    return tf_matrix

# Example usage of `term_frequency_matrix`
# Input: ["apple orange banana", "apple apple banana"], ['apple', 'orange', 'banana']
# Output: [[0.3333, 0.3333, 0.3333], [0.6667, 0, 0.3333]]
# Step 4: Calculate Document Frequency (DF) for each term
def document_frequency(term, corpus):
    return sum(1 for doc in corpus if term in tokenize(doc))

# Example usage of `document_frequency`
# Input: 'apple', ["apple orange banana", "apple apple banana", "grape pineapple"]
# Output: 2
# Step 5: Calculate Inverse Document Frequency (IDF) for each term
def inverse_document_frequency_matrix(corpus, vocabulary):
    N = len(corpus)
    idf_vector = []
    for term in vocabulary:
        df = document_frequency(term, corpus)
        idf = log(N / df) if df != 0 else 0  # Avoid division by zero
        idf_vector.append(idf)
    return idf_vector

# Example usage of `inverse_document_frequency_matrix`
# Input: ["apple orange banana", "apple apple banana", "grape pineapple"], ['apple', 'orange', 'banana', 'grape', 'pineapple']
# Output: [0.4055, 1.0986, 0.4055, 1.0986, 1.0986]
# Step 6: Calculate TF-IDF matrix by multiplying TF and IDF
def tfidf_matrix(tf_matrix, idf_vector):
    tfidf_matrix = []
    for tf_vector in tf_matrix:
        tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]
        tfidf_matrix.append(tfidf_vector)
    return tfidf_matrix

# Example usage of `tfidf_matrix`
# Input: [[0.3333, 0.3333, 0.3333], [0.6667, 0, 0.3333]], [0.4055, 1.0986, 0.4055]
# Output: [[0.1352, 0.3662, 0.1352], [0.2703, 0.0, 0.1352]]
# Heatmap visualization
def plot_heatmap(df, title):
    plt.figure(figsize=(10, 6))
    sns.heatmap(df, annot=True, fmt=".2f", cmap="plasma", cbar=True)
    plt.title(title, fontsize=16)
    plt.xlabel("Terms", fontsize=12)
    plt.ylabel("Documents", fontsize=12)
    plt.show()
# Experiment Scenarios
corpora = {
    "Unique Values": [
        "apple orange banana",
        "grape pineapple mango",
        "peach plum berry"
    ],
    "Similar Documents": [
        "car road fast",
        "car road blue",
        "car road driven"
    ],
    "Repeated Word": [
        "car car car car car",
        "car car driven road",
        "car car highway car car"
    ]
}
# Perform calculations for each corpus
for scenario, corpus in corpora.items():
    print(f"\n--- Scenario: {scenario} ---\n")

    # Create vocabulary
    vocabulary = create_vocabulary(corpus)

    # Calculate TF, IDF, and TF-IDF
    tf_matrix = term_frequency_matrix(corpus, vocabulary)
    idf_vector = inverse_document_frequency_matrix(corpus, vocabulary)
    tfidf = tfidf_matrix(tf_matrix, idf_vector)

    # Convert to DataFrames for display
    df_tf = pd.DataFrame(tf_matrix, columns=vocabulary)
    df_idf = pd.DataFrame([idf_vector], columns=vocabulary)
    df_tfidf = pd.DataFrame(tfidf, columns=vocabulary)

    # Display data
    print("Vocabulary:", vocabulary)
    print("\nTerm Frequency (TF):")
    print(df_tf)
    print("\nInverse Document Frequency (IDF):")
    print(df_idf)
    print("\nTF-IDF:")
    print(df_tfidf)

    # Visualize TF-IDF matrix
    plot_heatmap(df_tfidf, f"TF-IDF Heatmap - {scenario}")



ASSIGN 7 - Analytical representation of Linear regression using Movie  ecommendation dataset


# #movies_data <- data.frame(
# #  MovieID = 1:10,
# #  Genre = factor(c("Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action", "Comedy", "Drama", "Action")),
# #  Budget = c(15000000, 5000000, 10000000, 20000000, 6000000, 12000000, 18000000, 5500000, 11000000, 16000000),
# #  Rating = c(7.8, 6.5, 8.2, 7.9, 6.9, 8.0, 7.5, 6.7, 8.1, 7.7)
# #)
# 
# movies_data <-load("movies.Rdata")
# #write.csv(movies_data, file = "movies_data.csv", row.names = FALSE)
# 
# print(movies_data)
# 
# install.packages("ggplot2")
# install.packages("dplyr")
# 
# library(ggplot2)
# library(dplyr)
# 
# 
# head(movies)
# 
# str(movies)
# 
# movies$genre <- as.factor(movies$genre)
# 
# summary(movies)
# 
# set.seed(123)
# 
# # Create a training set (70%) and test set (30%)
# sample_indices <- sample(seq_len(nrow(movies)), size = 0.7 * nrow(movies))
# train_set <- movies[sample_indices, ]
# test_set <- movies[-sample_indices, ]
# 
# model <- lm(imdb_rating ~ genre, data = train_set)
# 
# # Summary of the model
# summary(model)
# 
# predictions <- predict(model, newdata = test_set)
# 
# # Combine predictions with actual values
# results <- data.frame(Actual = test_set$imdb_rating, Predicted = predictions)
# 
# # Calculate Mean Squared Error
# mse <- mean((results$Actual - results$Predicted)^2)
# print(paste("Mean Squared Error:", mse))
# 
# 
# ggplot(results, aes(x = Actual, y = Predicted)) +
#   geom_point() +
#   geom_abline(intercept = 0, slope = 1, color = "red") +
#   labs(title = "Actual vs Predicted Ratings", x = "Actual Rating", y = "Predicted Rating")
# 
# 
# residuals <- results$Actual - results$Predicted
# 
# ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
#   geom_histogram(bins = 30, fill = "blue", color = "black") +
#   labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency")
# 
# 
# 
# 

# Load required libraries
install.packages("ggplot2", dependencies = TRUE)
install.packages("dplyr", dependencies = TRUE)

library(ggplot2)
library(dplyr)

# Load dataset
load("movies.Rdata")

# Inspect the data
str(movies)
summary(movies)

# Ensure Genre is a factor
movies$genre <- as.factor(movies$genre)

# Split data into training and test sets (70/30 split)
set.seed(123)
sample_indices <- sample(seq_len(nrow(movies)), size = 0.7 * nrow(movies))
train_set <- movies[sample_indices, ]
test_set <- movies[-sample_indices, ]

# Build the linear regression model
model <- lm(imdb_rating ~ genre, data = train_set)

# Model summary
summary(model)

# Predict on test set
predictions <- predict(model, newdata = test_set)

# Combine actual and predicted values
results <- data.frame(Actual = test_set$imdb_rating, Predicted = predictions)

# Evaluate model performance
mse <- mean((results$Actual - results$Predicted)^2)
rmse <- sqrt(mse)
mae <- mean(abs(results$Actual - results$Predicted))

cat("Model Evaluation Metrics:\n")
cat(paste("Mean Squared Error (MSE):", round(mse, 3), "\n"))
cat(paste("Root Mean Squared Error (RMSE):", round(rmse, 3), "\n"))
cat(paste("Mean Absolute Error (MAE):", round(mae, 3), "\n"))

# Plot Actual vs Predicted ratings
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted Ratings", x = "Actual Rating", y = "Predicted Rating") +
  theme_minimal()

# Residual analysis
residuals <- results$Actual - results$Predicted

# Histogram of residuals
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Residuals vs Fitted Values plot for diagnostics
ggplot(data.frame(Fitted = predictions, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "darkorange", alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

#LR USING MTCARS

# Load required libraries
install.packages("ggplot2", dependencies = TRUE)
install.packages("dplyr", dependencies = TRUE)

library(ggplot2)
library(dplyr)

# Load the mtcars dataset
data(mtcars)

# Inspect the dataset
str(mtcars)
summary(mtcars)

# Split data into training (70%) and test (30%) sets
set.seed(123)
sample_indices <- sample(seq_len(nrow(mtcars)), size = 0.7 * nrow(mtcars))
train_set <- mtcars[sample_indices, ]
test_set <- mtcars[-sample_indices, ]

# Build the linear regression model to predict mpg
model <- lm(mpg ~ hp + wt + qsec, data = train_set)

# Model summary
summary(model)

# Predict on test set
predictions <- predict(model, newdata = test_set)

# Combine actual and predicted values
results <- data.frame(Actual = test_set$mpg, Predicted = predictions)

# Evaluate model performance
mse <- mean((results$Actual - results$Predicted)^2)
rmse <- sqrt(mse)
mae <- mean(abs(results$Actual - results$Predicted))

cat("Model Evaluation Metrics:\n")
cat(paste("Mean Squared Error (MSE):", round(mse, 3), "\n"))
cat(paste("Root Mean Squared Error (RMSE):", round(rmse, 3), "\n"))
cat(paste("Mean Absolute Error (MAE):", round(mae, 3), "\n"))

# Plot Actual vs Predicted mpg values
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(title = "Actual vs Predicted MPG", x = "Actual MPG", y = "Predicted MPG") +
  theme_minimal()

# Residual analysis
residuals <- results$Actual - results$Predicted

# Histogram of residuals
ggplot(data.frame(Residuals = residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Residuals vs Fitted Values plot for diagnostics
ggplot(data.frame(Fitted = predictions, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
  geom_point(color = "darkorange", alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
  theme_minimal()



ASSIGN 8 - Data streaming and pipelining using online cloud platform using confluent for Kafka cloud

https://www.youtube.com/watch?v=oI7VAS9KSS4

https://drive.google.com/drive/folders/16vkmCspZoAIix9vr5aBcBTK6dX-JJ5gL?usp=sharing


ASSIGN 9 - Social Network Analysis using R (for example: Community Detection Algorithm)

install.packages("igraph")
install.packages("sna")

library(igraph)
library(network)
library(sna)

plot(make_full_graph(8, directed=FALSE))

Ring_Graph <- make_ring(12, directed = FALSE, mutual =FALSE, circular = TRUE)
# try for False
plot(Ring_Graph)

Star_Graph <- make_star(12, center = 4)
plot(Star_Graph)

gnp_Graph <- sample_gnp(20, 0.5, directed = FALSE, loops =FALSE)
plot(gnp_Graph)

gnp_Graph1 <- sample_gnp(7, 0.4, directed = FALSE, loops = FALSE)
plot(gnp_Graph1)

node_degrees <- degree(gnp_Graph)
print(node_degrees)

sample_graph <- sample_gnp(10, 0.3, directed = FALSE)
plot(sample_graph)
sample_density <- edge_density(sample_graph, loops = FALSE)
sample_density

sample_graph <- sample_gnp(20, 0.3, directed = FALSE, loops= FALSE)
plot(sample_graph)
clique_num(sample_graph)

components(sample_graph)


setwd("D:\\MIT ADT\\LY - Sem 1\\BDA Lab\\Amreen Mam\\Assign 9")
data <- read.csv("socialnetworkdata.csv")
y <- data.frame(data$first, data$second)
net <- graph.data.frame(y, directed=T)
V(net)
E(net)
plot(net)

hist(degree(net), col='purple', main = "histogram of node degree",ylab = 'freq', xlab = 'vertices degree')


set.seed(222)
plot(net,
     vertex.color = 'cyan',
     vertext.size = 2,
     edge.arrow.size = 0.1,
     vertex.label.cex = 0.8)


plot(net,
     vertex.color = rainbow(vcount(net)),  # Use the number of vertices to generate colors
     vertex.size = degree(net) * 0.4,      # Calculate vertex degrees and scale their size
     edge.arrow.size = 0.1,                # Keep arrow size for edges
     layout = layout.fruchterman.reingold)


hs <- hub_score(net)$vector
hs
as <- authority_score(net)$vector
as
set.seed(123)
plot(net,
     vertex.size=hs*30,
     main = 'Hubs',
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai)


undirected_net <- as.undirected(net, mode = "collapse")

community <- cluster_louvain(undirected_net)

plot(undirected_net,
     vertex.color = membership(community),  # Color vertices by community membership
     vertex.size = degree(undirected_net) * 0.4,  # Size vertices by degree
     edge.arrow.size = 0.1,  # Edge arrow size
     main = "Community Detection using Louvain Method")


# Initialization: Assign each node to its own community.
# Local Moving: For each node, attempt to move it to neighboring communities if it increases modularity (a measure of community structure quality).
# Community Aggregation: Treat each community as a "supernode" and create a new graph with these supernodes as nodes, where edge weights represent the strength of connections between communities.
# Repeat: Iterate the moving and aggregation steps on the new graph until modularity no longer increases.
# Output: The final community structure, with nodes grouped into communities maximizing modularity.


🤡







